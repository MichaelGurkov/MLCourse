---
title: "Homework2"
---

```{r include=FALSE}

knitr::opts_chunk$set(echo = FALSE, error = FALSE, warning = FALSE, message = FALSE)

```

```{r load_libraries}

library(tidyverse)

library(tidymodels)

```

```{r load_data}

wine = read_csv(here::here("data","winequality_red.csv"))

heart = read_csv(here::here("data","heart.csv"))

```

# Preface

We can use the data for prediction without assumptions because we don't care about
causality. If cigar smoking can predict better health I don't care that this is because
cigar smokers are wealthy people who can afford better treatment.

## Separability

The downside in adding interactions is that if we got the interaction wrong we'll cause overfitting.

## Normal distribution
1. Classical economist's stories such as consumption that is characterized by different
dispersion (variance) as a function of income levels
2. If we understand the structure and effect of the "noise" we can for on opinion of how
much trust can be given to the results.



# Data

```{r plot_hist_wine}
wine %>% 
  pivot_longer(cols = everything()) %>% 
  ggplot(aes(x = value)) + 
  geom_histogram() + 
  facet_wrap(~name, scales = "free")

```


```{r plot_boxplot_wine}

wine %>% 
  mutate(quality = as_factor(quality)) %>% 
  pivot_longer(cols = -quality) %>%
  ggplot(aes(x = value, y = quality)) + 
  geom_boxplot() + 
  facet_wrap(~name, scales = "free")

```



# Model

## Linear regression


```{r split_lin_model}

lin_model_split = initial_split(wine,prop = 0.7, strata = quality)

wine_train = lin_model_split %>% 
  training()

wine_test = lin_model_split %>% 
  testing()

```


```{r fit_lin_model}

reg_model = linear_reg() %>%
  set_engine("lm") %>%
  fit(quality ~ .,wine_train)


reg_model %>%tidy()

```

```{r predict_lin_model}

reg_model %>% 
  predict(wine_test) %>%
  head()

```

```{r evaluate_lin_model}

reg_model %>%
  predict(wine_test) %>% 
  bind_cols(truth = wine_test$quality) %>% 
  metrics(truth = truth, estimate = .pred)

```

The main difference between confidence intervals and RMSE evaluation is that
RMSE doesn't depended on the assumptions of the underlying DGP



## Logistic regression

Using linear regression for binary outcomes may produce predictions outside of
[0,1] boundaries. Another side effect is that itcreates a 
heteroscedasticity problem

```{r plot_hist_heart}
heart %>% 
  pivot_longer(cols = everything()) %>% 
  ggplot(aes(x = value)) + 
  geom_histogram() + 
  facet_wrap(~name, scales = "free")

```


```{r split_log_model}

set.seed(100)

log_model_split = initial_split(heart,prop = 0.7, strata = target)

heart_train = log_model_split %>% 
  training()

heart_test = log_model_split %>% 
  testing()




```

```{r recipes}

heart_log_rec = recipe(target ~ .,data = heart_train) %>% 
  step_mutate(target = factor(target, levels = c("1","0")))

heart_lin_rec = recipe(target ~ .,data = heart_train)

```


```{r models}

heart_log_mod = logistic_reg() %>%
  set_engine("glm")

heart_lin_mod = linear_reg() %>%
  set_engine("lm")


heart_ridge_mod = linear_reg(penalty = tune(),mixture = 0) %>%
  set_engine("lm")


```


```{r fit_lin_heart_model}

lin_mod_fit = workflow() %>% 
  add_recipe(heart_lin_rec) %>% 
  add_model(heart_lin_mod) %>% 
  fit(heart_train)



lin_mod_fit %>% 
  pull_workflow_fit() %>%
  tidy()

```

```{r predict_lin_heart_model}

lin_mod_fit %>% 
  pull_workflow_fit() %>% 
  predict(heart_test) %>% 
  pull(.pred) %>% 
  range()

```

We can see that the predictions are indeed outside of [0,1] boundaries.

```{r plot_lin_roc_curve}

lin_mod_fit %>% 
  pull_workflow_fit() %>% 
  predict(heart_test) %>% 
  mutate(norm_pred = scales::rescale(.pred)) %>% 
  bind_cols(target = heart_test$target) %>% 
  mutate(target = factor(target, levels = c(1,0))) %>% 
  select(truth = target,norm_pred) %>% 
  roc_curve(truth = truth, norm_pred) %>% 
  autoplot()


```


```{r fit_log_heart_model}

log_mod_fit = workflow() %>% 
  add_recipe(heart_log_rec) %>% 
  add_model(heart_log_mod) %>% 
  fit(heart_train)


log_mod_fit %>% 
  pull_workflow_fit() %>% 
  broom::tidy()

```

```{r predict_log_heart_model}

log_mod_fit %>% 
  pull_workflow_fit() %>%  
  predict(heart_test, type = "prob") %>% 
  pull(.pred_1) %>% 
  range()

```

We can see that the predictions of logistic model are within the [0,1] boundaries.




## Ridge regression

```{r}

heart_ridge_mod = linear_reg(penalty = tune(),mixture = 0) %>%
  set_engine("glmnet")

ridge_wf = workflow() %>% 
  add_recipe(heart_lin_rec) %>% 
  add_model(heart_ridge_mod)

penalty_grid = grid_regular(penalty(), levels = 50)

ridge_cv = tune_grid(ridge_wf, grid = penalty_grid,
                     resamples = vfold_cv(heart_train, v = 5))
ridge_cv %>% 
  collect_metrics() %>% 
  filter(.metric == "rmse") %>% 
  ggplot(aes(x = penalty, y = mean)) + 
  geom_line() + 
  geom_point() + 
  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err)) + 
  scale_x_log10()


```

