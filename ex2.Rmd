---
title: "Homework2"
---

```{r include=FALSE}

knitr::opts_chunk$set(echo = FALSE, error = FALSE, warning = FALSE, message = FALSE)

```

```{r load_libraries}

library(tidyverse)

library(tidymodels)

```

```{r load_data}

wine = read_csv(here::here("data","winequality_red.csv"))

heart = read_csv(here::here("data","heart.csv"))

```

# Preface

We can use the data for prediction without assumptions because we don't care about
causality. If cigar smoking can predict better health I don't care that this is because
cigar smokers are wealthy people who can afford better treatment.

## Separability

The downside in adding interactions is that if we got the interaction wrong we'll cause overfitting.

## Normal distribution
1. Classical economist's stories such as consumption that is characterized by different
dispersion (variance) as a function of income levels
2. If we understand the structure and effect of the "noise" we can for on opinion of how
much trust can be given to the results.



# Data

```{r plot_hist_wine}
wine %>% 
  pivot_longer(cols = everything()) %>% 
  ggplot(aes(x = value)) + 
  geom_histogram() + 
  facet_wrap(~name, scales = "free")

```


```{r plot_boxplot_wine}

wine %>% 
  mutate(quality = as_factor(quality)) %>% 
  pivot_longer(cols = -quality) %>%
  ggplot(aes(x = value, y = quality)) + 
  geom_boxplot() + 
  facet_wrap(~name, scales = "free")

```



# Model

## Linear regression


```{r split_lin_model}

lin_model_split = initial_split(wine,prop = 0.7, strata = quality)

wine_train = lin_model_split %>% 
  training()

wine_test = lin_model_split %>% 
  testing()

```


```{r fit_lin_model}

reg_model = linear_reg() %>%
  set_engine("lm") %>%
  fit(quality ~ .,wine_train)


reg_model %>% 
  broom::tidy()

```

```{r predict_lin_model}

reg_model %>% 
  predict(wine_test) %>%
  head()

```

```{r evaluate_lin_model}

reg_model %>%
  predict(wine_test) %>% 
  bind_cols(truth = wine_test$quality) %>% 
  metrics(truth = truth, estimate = .pred)

```

The main difference between confidence intervals and RMSE evaluation is that
RMSE doesn't depended on the assumptions of the underlying DGP



## Logistic regression

Using linear regression for binary outcomes may produce predictions outside of
[0,1] boundaries. Another side effect is that itcreates a 
heteroscedasticity problem

```{r plot_hist_heart}
heart %>% 
  pivot_longer(cols = everything()) %>% 
  ggplot(aes(x = value)) + 
  geom_histogram() + 
  facet_wrap(~name, scales = "free")

```


```{r split_log_model}

set.seed(100)

log_model_split = initial_split(heart,prop = 0.7, strata = target)

heart_train = log_model_split %>% 
  training()

heart_test = log_model_split %>% 
  testing()

```


```{r fit_lin_heart_model}

heart_lin_model = linear_reg() %>%
  set_engine("lm") %>%
  fit(target ~ .,heart_train)

heart_lin_model %>% 
  broom::tidy()

```

```{r predict_lin_heart_model}

heart_lin_model %>% 
  predict(heart_test) %>% 
  pull(.pred) %>% 
  range()

```

We can see that the predictions are indeed outside of [0,1] boundaries.

```{r plot_lin_roc_curve}

heart_lin_model %>% 
  predict(heart_test) %>% 
  mutate(norm_pred = scales::rescale(.pred)) %>% 
  bind_cols(target = heart_test$target) %>% 
  mutate(target = factor(target, levels = c(1,0))) %>% 
  select(truth = target,norm_pred) %>% 
  roc_curve(truth = truth, norm_pred) %>% 
  autoplot()


```


```{r fit_log_heart_model}

heart_log_model = logistic_reg() %>% 
  set_engine("glm") %>%
  fit(target ~ .,heart_train %>% 
        mutate(target = as_factor(target)))

heart_log_model %>% 
  broom::tidy()

```

```{r predict_log_heart_model}

heart_log_model %>% 
  predict(heart_test %>% 
            mutate(target = as_factor(target)), type = "prob") %>% 
  pull(.pred_1) %>% 
  range()

```

We can see that the predictions of logistic model are within the [0,1] boundaries.
