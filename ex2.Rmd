---
title: "Homework2"
---

```{r include=FALSE}

knitr::opts_chunk$set(echo = FALSE, error = FALSE, warning = FALSE, message = FALSE)

```

```{r load_libraries}

library(tidyverse)

library(tidymodels)

library(glmnet)

library(GGally)

```

```{r load_data}

wine = read_csv(here::here("data","winequality_red.csv"))

heart = read_csv(here::here("data","heart.csv"))

```

# Preface

### Exercises

1. We can use the data for prediction without assumptions because in that case
we don't care much about causality. If cigar smoking can predict better health,we don't care that this is may be because cigar smokers are wealthy people who can afford better treatment.

## Separability

### Exercises

1. The downside in adding interactions is that if we got the interaction wrong we'll cause overfitting.

## Normal distribution

### Exercises

1. Classical economist's stories such as consumption that is characterized by different
dispersion (variance) as a function of income levels
2. If we understand the structure and effect of the "noise" we can form an
opinion of how much trust can be given to the results.



# Data

```{r plot_hist_wine, fig.cap="Wine dataset histplot"}
wine %>% 
  pivot_longer(cols = everything()) %>% 
  ggplot(aes(x = value)) + 
  geom_histogram() + 
  facet_wrap(~name, scales = "free")

```


```{r plot_boxplot_wine, fig.cap="Wine dataset boxplot"}

wine %>% 
  mutate(quality = as_factor(quality)) %>% 
  pivot_longer(cols = -quality) %>%
  ggplot(aes(x = value, y = quality)) + 
  geom_boxplot() + 
  facet_wrap(~name, scales = "free")

```



# Model

## Linear regression

### Exercises

```{r split_lin_model}

lin_model_split = initial_split(wine,prop = 0.7, strata = quality)

wine_train = lin_model_split %>% 
  training()

wine_test = lin_model_split %>% 
  testing()

```

2.

```{r fit_lin_model}

reg_model = linear_reg() %>%
  set_engine("lm") %>%
  fit(quality ~ .,wine_train)


reg_model %>% 
  broom::tidy()

```

3.

```{r predict_lin_model}

reg_model %>% 
  predict(wine_test) %>%
  head()

```

4.

```{r evaluate_lin_model}

reg_model %>%
  predict(wine_test) %>% 
  bind_cols(truth = wine_test$quality) %>% 
  metrics(truth = truth, estimate = .pred)

```

The main difference between confidence intervals and RMSE evaluation is that
RMSE doesn't depended on the assumptions of the underlying DGP.




## Logistic regression

### Exersices

1. Using linear regression for binary outcomes may produce predictions outside of
[0,1] boundaries. Another side effect is that creates a heteroscedasticity
problem

2.

```{r plot_hist_heart, fig.cap="heart dataset histplot"}
heart %>% 
  pivot_longer(cols = everything()) %>% 
  ggplot(aes(x = value)) + 
  geom_histogram() + 
  facet_wrap(~name, scales = "free")

```
 
### Linear regression

#### Exercises

```{r split_log_model}

set.seed(100)

log_model_split = initial_split(heart,prop = 0.7, strata = target)

heart_train = log_model_split %>% 
  training()

heart_test = log_model_split %>% 
  testing()

```

2. 
```{r fit_lin_heart_model}

heart_lin_model = linear_reg() %>%
  set_engine("lm") %>%
  fit(target ~ .,heart_train)

heart_lin_model %>% 
  broom::tidy()

```

3.

```{r predict_lin_heart_model}

heart_lin_model %>% 
  predict(heart_test) %>% 
  pull(.pred) %>% 
  range()

```

We can see that the predictions are indeed outside of [0,1] boundaries.

4. 

```{r plot_lin_roc_curve}

heart_lin_model %>% 
  predict(heart_test) %>% 
  mutate(norm_pred = scales::rescale(.pred)) %>% 
  bind_cols(target = heart_test$target) %>% 
  mutate(target = factor(target, levels = c(1,0))) %>% 
  select(truth = target,norm_pred) %>% 
  roc_curve(truth = truth, norm_pred) %>% 
  autoplot()


```

### Logistic regression

#### Exercises

1.

```{r fit_log_heart_model}

heart_log_model = logistic_reg() %>% 
  set_engine("glm") %>%
  fit(target ~ .,heart_train %>% 
        mutate(target = as_factor(target)))

heart_log_model %>% 
  broom::tidy()

```

2.

```{r predict_log_heart_model}

heart_log_model %>% 
  predict(heart_test %>% 
            mutate(target = as_factor(target)), type = "prob") %>% 
  pull(.pred_1) %>% 
  range()

```

We can see that the predictions of logistic model are within the [0,1] boundaries.


## Regularization

### Exercises

1. Using an absolute (or squared term) in the penalty is required to ignore
the sign of the coefficients. In case the sign is taken
into account negative coefficients with be (inappropriately) rewarded.

2. This is a feature of the loss function. Absolute distance that is used in
lasso allows for "corner" solutions as opposed to quadratic distance used in
ridge

### Exercises


```{r fit_ridge_model}

ridge_mod = glmnet(
  x = heart_train %>%
    select(-target) %>%
    as.matrix(),
  y = heart_train %>%
    select(target) %>%
    as.matrix(),
  alpha = 0
)
ridge_cv = cv.glmnet(
  x = heart_train %>%
    select(-target) %>%
    as.matrix(),
  y = heart_train %>%
    select(target) %>%
    as.matrix(),
  alpha = 0
)


```

1. 

```{r plot_log_lambda_cv_coeffs}

plot(ridge_mod, xvar = "lambda")

```

2.

```{r plot_cv}

plot(ridge_cv, alpha = 0)

```

3. 

```{r compare_coeffs}

ridge_mod %>%
  tidy() %>%
  filter(step == max(step)) %>%
  select(term, ridge = estimate) %>%
  left_join(
    heart_log_model %>%
      tidy() %>%
      select(term, logit = estimate),
    by = "term") %>% 
  pivot_longer(-term) %>%
  ggplot(aes(x = term, y = value, fill = name)) +
  scale_fill_viridis_d() + 
  geom_col(position = "dodge") +
  theme(legend.title = element_blank())


```

We can see that ridge produces smaller (in absolute value) coeffs.

4. Perhaps the problem is that if we have a covariate that has no effect it
suggests that the economic model was misspecified?

5. 

```{r predict}

pred_results = predict(ridge_cv,
        newx = heart_test %>%
          select(-target) %>%
          as.matrix(),
        s = "lambda.min") %>%
  as_tibble() %>%
  set_names("min") %>%
  cbind(
    predict(
      ridge_cv,
      newx = heart_test %>%
        select(-target) %>%
        as.matrix(),
      s = "lambda.1se"
    ) %>%
      as_tibble() %>%
      set_names("se")
  ) %>%
  cbind(predict(heart_log_model, heart_test, type = "prob") %>%
          select(logit = .pred_1))

  

ggpairs(pred_results)

```
6.

Min lambda  conf matrix 

```{r min_conf_matrix}

min_preds = predict(ridge_cv,
        newx = heart_test %>%
          select(-target) %>%
          as.matrix(),
        s = "lambda.min", type = "response") %>%
  as_tibble() %>%
  set_names("pred") %>% 
  mutate(pred = factor(as.numeric(pred > 0.5), levels = c("1","0"))) %>% 
  bind_cols(heart_test %>% 
              mutate(truth = factor(target, levels = c(1,0))) %>% 
              select(truth))

caret::confusionMatrix(min_preds$pred,min_preds$truth)


```

One se lambda  conf matrix 

```{r se_conf_matrix}

se_preds = predict(ridge_cv,
        newx = heart_test %>%
          select(-target) %>%
          as.matrix(),
        s = "lambda.1se", type = "response") %>%
  as_tibble() %>%
  set_names("pred") %>% 
  mutate(pred = factor(as.numeric(pred > 0.5), levels = c("1","0"))) %>% 
  bind_cols(heart_test %>% 
              mutate(truth = factor(target, levels = c(1,0))) %>% 
              select(truth))

caret::confusionMatrix(se_preds$pred,se_preds$truth)


```
