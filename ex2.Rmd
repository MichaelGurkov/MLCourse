---
title: "Homework2"
---

```{r include=FALSE}

knitr::opts_chunk$set(echo = FALSE, error = FALSE, warning = FALSE, message = FALSE)

```

```{r load_libraries}

library(tidyverse)

library(tidymodels)

library(glmnet)

library(GGally)

```

```{r load_data}

wine = read_csv(here::here("data","winequality_red.csv"))

heart = read_csv(here::here("data","heart.csv"))

```

# Preface

We can use the data for prediction without assumptions because we don't care about
causality. If cigar smoking can predict better health I don't care that this is because
cigar smokers are wealthy people who can afford better treatment.

## Separability

The downside in adding interactions is that if we got the interaction wrong we'll cause overfitting.

## Normal distribution
1. Classical economist's stories such as consumption that is characterized by different
dispersion (variance) as a function of income levels
2. If we understand the structure and effect of the "noise" we can for on opinion of how
much trust can be given to the results.



# Data

```{r plot_hist_wine}
wine %>% 
  pivot_longer(cols = everything()) %>% 
  ggplot(aes(x = value)) + 
  geom_histogram() + 
  facet_wrap(~name, scales = "free")

```


```{r plot_boxplot_wine}

wine %>% 
  mutate(quality = as_factor(quality)) %>% 
  pivot_longer(cols = -quality) %>%
  ggplot(aes(x = value, y = quality)) + 
  geom_boxplot() + 
  facet_wrap(~name, scales = "free")

```



# Model

## Linear regression


```{r split_lin_model}

lin_model_split = initial_split(wine,prop = 0.7, strata = quality)

wine_train = lin_model_split %>% 
  training()

wine_test = lin_model_split %>% 
  testing()

```


```{r fit_lin_model}

reg_model = linear_reg() %>%
  set_engine("lm") %>%
  fit(quality ~ .,wine_train)


reg_model %>% 
  broom::tidy()

```

```{r predict_lin_model}

reg_model %>% 
  predict(wine_test) %>%
  head()

```

```{r evaluate_lin_model}

reg_model %>%
  predict(wine_test) %>% 
  bind_cols(truth = wine_test$quality) %>% 
  metrics(truth = truth, estimate = .pred)

```

The main difference between confidence intervals and RMSE evaluation is that
RMSE doesn't depended on the assumptions of the underlying DGP




## Logistic regression

Using linear regression for binary outcomes may produce predictions outside of
[0,1] boundaries. Another side effect is that itcreates a 
heteroscedasticity problem

```{r plot_hist_heart}
heart %>% 
  pivot_longer(cols = everything()) %>% 
  ggplot(aes(x = value)) + 
  geom_histogram() + 
  facet_wrap(~name, scales = "free")

```


```{r split_log_model}

set.seed(100)

log_model_split = initial_split(heart,prop = 0.7, strata = target)

heart_train = log_model_split %>% 
  training()

heart_test = log_model_split %>% 
  testing()

```


```{r fit_lin_heart_model}

heart_lin_model = linear_reg() %>%
  set_engine("lm") %>%
  fit(target ~ .,heart_train)

heart_lin_model %>% 
  broom::tidy()

```

```{r predict_lin_heart_model}

heart_lin_model %>% 
  predict(heart_test) %>% 
  pull(.pred) %>% 
  range()

```

We can see that the predictions are indeed outside of [0,1] boundaries.

```{r plot_lin_roc_curve}

heart_lin_model %>% 
  predict(heart_test) %>% 
  mutate(norm_pred = scales::rescale(.pred)) %>% 
  bind_cols(target = heart_test$target) %>% 
  mutate(target = factor(target, levels = c(1,0))) %>% 
  select(truth = target,norm_pred) %>% 
  roc_curve(truth = truth, norm_pred) %>% 
  autoplot()


```


```{r fit_log_heart_model}

heart_log_model = logistic_reg() %>% 
  set_engine("glm") %>%
  fit(target ~ .,heart_train %>% 
        mutate(target = as_factor(target)))

heart_log_model %>% 
  broom::tidy()

```

```{r predict_log_heart_model}

heart_log_model %>% 
  predict(heart_test %>% 
            mutate(target = as_factor(target)), type = "prob") %>% 
  pull(.pred_1) %>% 
  range()

```

We can see that the predictions of logistic model are within the [0,1] boundaries.



## Regularization

```{r fit_ridge_model}

ridge_mod = glmnet(
  x = heart_train %>%
    select(-target) %>%
    as.matrix(),
  y = heart_train %>%
    select(target) %>%
    as.matrix(),
  alpha = 0
)
ridge_cv = cv.glmnet(
  x = heart_train %>%
    select(-target) %>%
    as.matrix(),
  y = heart_train %>%
    select(target) %>%
    as.matrix(),
  alpha = 0
)


```

```{r plot_log_lambda_cv_coeffs}

plot(ridge_mod, xvar = "lambda")

```

```{r plot_cv}

plot(ridge_cv, alpha = 0)

```

```{r compare_coeffs}

ridge_mod %>%
  tidy() %>%
  filter(step == max(step)) %>%
  select(term, ridge = estimate) %>%
  left_join(
    heart_log_model %>%
      tidy() %>%
      select(term, logit = estimate),
    by = "term") %>% 
  pivot_longer(-term) %>%
  ggplot(aes(x = term, y = value, fill = name)) +
  scale_fill_viridis_d() + 
  geom_col(position = "dodge") +
  theme(legend.title = element_blank())


```

```{r predict}

pred_results = predict(ridge_cv,
        newx = heart_test %>%
          select(-target) %>%
          as.matrix(),
        s = "lambda.min") %>%
  as_tibble() %>%
  set_names("min") %>%
  cbind(
    predict(
      ridge_cv,
      newx = heart_test %>%
        select(-target) %>%
        as.matrix(),
      s = "lambda.1se"
    ) %>%
      as_tibble() %>%
      set_names("se")
  ) %>%
  cbind(predict(heart_log_model, heart_test, type = "prob") %>%
          select(logit = .pred_1))

  
pred_results %>% 
  rownames_to_column() %>% 
  pivot_longer(-rowname) %>% 
  ggplot(aes(x = as.numeric(rowname), y = value, color = name)) + 
  geom_point()
    

ggpairs(pred_results)

```

```{r min_conf_matrix}

min_preds = predict(ridge_cv,
        newx = heart_test %>%
          select(-target) %>%
          as.matrix(),
        s = "lambda.min", type = "response") %>%
  as_tibble() %>%
  set_names("pred") %>% 
  mutate(pred = factor(as.numeric(pred > 0.5), levels = c("1","0"))) %>% 
  bind_cols(heart_test %>% 
              mutate(truth = factor(target, levels = c(1,0))) %>% 
              select(truth))

caret::confusionMatrix(min_preds$pred,min_preds$truth)


```

```{r se_conf_matrix}

se_preds = predict(ridge_cv,
        newx = heart_test %>%
          select(-target) %>%
          as.matrix(),
        s = "lambda.1se", type = "response") %>%
  as_tibble() %>%
  set_names("pred") %>% 
  mutate(pred = factor(as.numeric(pred > 0.5), levels = c("1","0"))) %>% 
  bind_cols(heart_test %>% 
              mutate(truth = factor(target, levels = c(1,0))) %>% 
              select(truth))

caret::confusionMatrix(se_preds$pred,se_preds$truth)


```
